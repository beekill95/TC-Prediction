#!/bin/env python3

"""
This script creates labels that can be used to train deep learning model to
predict whether there will be tropical cyclones or not.
This is the second version of the script `create_labels.py`,
basically, it will:
    * Only work with ibtracs data.
    * Work with data generated by new extract python script.
    * No configuration file needed.
"""

import argparse
from datetime import datetime, timedelta
import glob
import os
import pandas as pd
from tqdm import tqdm
import xarray as xr


def parse_arguments(args=None):
    parser = argparse.ArgumentParser()

    parser.add_argument(
        '--best-track',
        dest='best_track',
        action='store',
        required=True,
        help=''' Path to ibtracs .csv file.
        ''')

    parser.add_argument(
        '--observations-dir',
        dest='observations_dir',
        action='store',
        required=True,
        help='''
            Path to directory contains all observation files .nc and .conf file.
            This is also the directory that the will contain the output csv file.
            The output file will be `tc_{lead_time}h_{basins}.csv`.
            ''')

    parser.add_argument(
        '--leadtime',
        dest='leadtime',
        default=0,
        type=int,
        help='The lead time to generate the data. Default is 0h.')

    return parser.parse_args(args)


def parse_date_from_nc_filename(filename: str):
    FMT = '%Y%m%d_%H_%M'
    filename, _ = os.path.splitext(os.path.basename(filename))
    datepart = '_'.join(filename.split('_')[1:])
    return datetime.strptime(datepart, FMT)


def list_reanalysis_files(path: str) -> pd.DataFrame:
    files = glob.iglob(os.path.join(path, '*.nc'))
    files = ((parse_date_from_nc_filename(f), f) for f in files)
    dates, filepaths = zip(*files)
    return pd.DataFrame({
        'Date': dates,
        'Path': filepaths
    })


def load_best_track(
        path: str,
        domain: tuple[float, float, float, float]) -> tuple[pd.DataFrame, pd.DataFrame]:
    def filter_by_domain(df: pd.DataFrame):
        latmin, latmax, lonmin, lonmax = domain
        lon_mask = (df['LON'] >= lonmin) & (df['LON'] <= lonmax)
        lat_mask = (df['LAT'] >= latmin) & (df['LAT'] <= latmax)
        return df[lon_mask & lat_mask]

    df = pd.read_csv(path, skiprows=(1,), na_filter=False)
    # Parse data column.
    df['Date'] = pd.to_datetime(
        df['ISO_TIME'], format='%Y-%m-%d %H:%M:%S')

    # Convert Longitude.
    # TODO: check this.
    df['LON'] = df['LON'].apply(lambda l: l if l < 180 else 360 - l)

    # Group by SID, and only retain the first row.
    genesis_df = df.groupby('SID', sort=False).first()
    genesis_df = genesis_df.copy()
    genesis_df['SID'] = genesis_df.index

    return filter_by_domain(genesis_df), filter_by_domain(df)


def create_labels(file_genesis_df, best_track_df) -> pd.DataFrame:
    def has_genesis(row: pd.Series):
        return True if row['SID'] else False

    def last_observed(row: pd.Series):
        sid = best_track_df['SID']
        df = best_track_df[sid == row['SID']]
        return df['Date'].iloc[-1]

    def other_tc(row: pd.Series):
        original_file_date = row['OriginalDate']
        date_mask = best_track_df['Date'] == original_file_date


    rows = []
    for _, row in tqdm(file_genesis_df.iterrows()):
        label = {
            'Date': row['Date'],
            'Genesis': has_genesis(row),
            'TC': has_genesis(row),
            'TC Id': row['SID'],
            'Longitude': row['LON'],
            'Latitude': row['LAT'],
            'First Observed': row['Date'],
            'Last Observed': last_observed(row),
            'First Observed Type': row['Nature'],
            'Will Develop to TC': 'TODO',
            'Developing Date': 'TODO',
            'Is Other TC Happening': 'TODO',
            'Other TC Locations': 'TODO',
            'Path': row['Path'],
        }
        rows.append(label)

    return pd.DataFrame(rows)


def get_domain(path: str) -> tuple[float, float, float, float]:
    ds = xr.load_dataset(path)
    lat = ds['lat']
    lon = ds['lon']
    return lat.min(), lat.max(), lon.min(), lon.max()


def main(args=None):
    args = parse_arguments(args)

    # List reanalysis files.
    files_df = list_reanalysis_files(args.observations_dir)
    files_df['OriginalDate'] = files_df['Date'].copy()
    files_df['Date'] = files_df['Date'].apply(
        lambda d: d + timedelta(hours=args.leadtime))

    files_df = files_df.sort_values('Date')

    genesis_df, best_track_df = load_best_track(args.best_track)

    # Merge these two dataframes.
    files_genesis_df = files_df.merge(genesis_df, how='left', on='Date')
    assert len(files_df) == len(files_genesis_df), 'Just to make sure we dont remove rows from files_df'

    labels_df = create_labels(files_genesis_df, best_track_df)
    output_path = os.path.join(
        args.observations_dir, f'tc_{args.leadtime}h.csv')
    labels_df.to_csv(output_path)

if __name__ == '__main__':
    main()
